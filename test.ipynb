{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "import time\n",
    "\n",
    "data = pd.read_csv(\"df_UK_lat_lon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'RLOIid', 'catchmentName', 'dateOpened', 'datumOffset', 'easting',\n",
       "       'label', 'lat', 'long', 'northing', 'notation', 'riverName',\n",
       "       'stageScale', 'stationReference', 'status', 'town', 'wiskiID',\n",
       "       'measures'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error geocoding nan, nan: Must be a coordinate pair or Point\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = data\n",
    "\n",
    "# Initialize the geolocator\n",
    "geolocator = Nominatim(user_agent=\"town_filler\")\n",
    "\n",
    "# Step 1: Identify unique (lat, long) pairs where town is missing\n",
    "missing_towns = df[df['town'].isna()][['lat', 'long']].drop_duplicates()\n",
    "\n",
    "# Step 2: Dictionary to store geocoded towns\n",
    "town_dict = {}\n",
    "\n",
    "# Step 3: Geocode each unique coordinate pair with a delay\n",
    "for index, row in missing_towns.iterrows():\n",
    "    lat = row['lat']\n",
    "    long = row['long']\n",
    "    try:\n",
    "        # Perform reverse geocoding\n",
    "        location = geolocator.reverse((lat, long))\n",
    "        if location and 'address' in location.raw:\n",
    "            address = location.raw['address']\n",
    "            # Extract town, city, or village (whichever is available)\n",
    "            town = address.get('town') or address.get('city') or address.get('village')\n",
    "            town_dict[(lat, long)] = town\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {lat}, {long}: {e}\")\n",
    "    # Manual delay to respect service limits (1 request per second)\n",
    "    time.sleep(1)\n",
    "\n",
    "# Step 4: Function to fill missing towns\n",
    "def fill_town(row):\n",
    "    if pd.isna(row['town']) or row['town'] == '':\n",
    "        key = (row['lat'], row['long'])\n",
    "        # Return the geocoded town if available, else keep original\n",
    "        return town_dict.get(key, row['town'])\n",
    "    return row['town']\n",
    "\n",
    "# Step 5: Apply the function to the DataFrame\n",
    "df['town'] = df.apply(fill_town, axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_request = requests.get('https://environment.data.gov.uk/flood-monitoring/id/stations')\n",
    "read = data_request.json()\n",
    "temp_lis = []\n",
    "for i in read['items']:\n",
    "    temp_lis.append(i)\n",
    "columns = []\n",
    "for j in i.keys():\n",
    "    j = str(j)\n",
    "    columns.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'uk.geojson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muk.geojson\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     geojson_data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'uk.geojson'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('uk.geojson', 'r') as f:\n",
    "    geojson_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  id RLOIid  \\\n",
      "0  http://environment.data.gov.uk/flood-monitorin...   7041   \n",
      "1  http://environment.data.gov.uk/flood-monitorin...   6022   \n",
      "2  http://environment.data.gov.uk/flood-monitorin...   3072   \n",
      "3  http://environment.data.gov.uk/flood-monitorin...   6177   \n",
      "4  http://environment.data.gov.uk/flood-monitorin...   2032   \n",
      "\n",
      "                             catchmentName  dateOpened  datumOffset easting  \\\n",
      "0                                Cotswolds  1994-01-01          NaN  417990   \n",
      "1                                  Welland  1992-01-01          2.0  528000   \n",
      "2  Parrett, Brue and West Somerset Streams  1997-01-01         10.0  344383   \n",
      "3                   Upper and Bedford Ouse  1996-10-01          6.3  529500   \n",
      "4             Worcestershire Middle Severn  1976-04-06          NaN  386120   \n",
      "\n",
      "             label        lat      long northing notation         riverName  \\\n",
      "0  Bourton Dickler  51.874767 -1.740083   219610   1029TH      River Dikler   \n",
      "1  Surfleet Sluice  52.845991 -0.100848   329300    E2043        River Glen   \n",
      "2       Gaw Bridge  50.976043 -2.793549   119926    52119     River Parrett   \n",
      "3       Hemingford  52.323618 -0.101287   271200   E21136  River Great Ouse   \n",
      "4          Swindon   52.51274 -2.205945   290560     2067     Smestow Brook   \n",
      "\n",
      "                                          stageScale stationReference  \\\n",
      "0  http://environment.data.gov.uk/flood-monitorin...           1029TH   \n",
      "1  http://environment.data.gov.uk/flood-monitorin...            E2043   \n",
      "2  http://environment.data.gov.uk/flood-monitorin...            52119   \n",
      "3  http://environment.data.gov.uk/flood-monitorin...           E21136   \n",
      "4  http://environment.data.gov.uk/flood-monitorin...             2067   \n",
      "\n",
      "                                              status                town  \\\n",
      "0  http://environment.data.gov.uk/flood-monitorin...   Little Rissington   \n",
      "1  http://environment.data.gov.uk/flood-monitorin...   Surfleet Seas End   \n",
      "2  http://environment.data.gov.uk/flood-monitorin...  Kingsbury Episcopi   \n",
      "3  http://environment.data.gov.uk/flood-monitorin...     Hemingford Grey   \n",
      "4  http://environment.data.gov.uk/flood-monitorin...             Swindon   \n",
      "\n",
      "     wiskiID                                           measures  \n",
      "0     1029TH  [{\"@id\": \"http://environment.data.gov.uk/flood...  \n",
      "1     L31004  [{\"@id\": \"http://environment.data.gov.uk/flood...  \n",
      "2  520320_FW  [{\"@id\": \"http://environment.data.gov.uk/flood...  \n",
      "3     L33865  [{\"@id\": \"http://environment.data.gov.uk/flood...  \n",
      "4       2067  [{\"@id\": \"http://environment.data.gov.uk/flood...  \n"
     ]
    }
   ],
   "source": [
    "data = temp_lis\n",
    "\n",
    "flattened_data = []\n",
    "for station in data:\n",
    "    station_dict = {\n",
    "        'id': station.get('@id'),\n",
    "        'RLOIid': station.get('RLOIid'),\n",
    "        'catchmentName': station.get('catchmentName'),\n",
    "        'dateOpened': station.get('dateOpened'),\n",
    "        'datumOffset': station.get('datumOffset'),\n",
    "        'easting': station.get('easting'),\n",
    "        'label': station.get('label'),\n",
    "        'lat': station.get('lat'),\n",
    "        'long': station.get('long'),\n",
    "        'northing': station.get('northing'),\n",
    "        'notation': station.get('notation'),\n",
    "        'riverName': station.get('riverName'),\n",
    "        'stageScale': station.get('stageScale'),\n",
    "        'stationReference': station.get('stationReference'),\n",
    "        'status': station.get('status'),\n",
    "        'town': station.get('town'),\n",
    "        'wiskiID': station.get('wiskiID'),\n",
    "        'measures': json.dumps(station.get('measures', []))  # Convert measures list to string\n",
    "    }\n",
    "    flattened_data.append(station_dict)\n",
    "\n",
    "df = pd.DataFrame(flattened_data)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mnunique()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:11836\u001b[0m, in \u001b[0;36mDataFrame.nunique\u001b[0;34m(self, axis, dropna)\u001b[0m\n\u001b[1;32m  11798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnunique\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m  11799\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m  11800\u001b[0m \u001b[38;5;124;03m    Count number of distinct elements in specified axis.\u001b[39;00m\n\u001b[1;32m  11801\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11834\u001b[0m \u001b[38;5;124;03m    dtype: int64\u001b[39;00m\n\u001b[1;32m  11835\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m> 11836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(Series\u001b[38;5;241m.\u001b[39mnunique, axis\u001b[38;5;241m=\u001b[39maxis, dropna\u001b[38;5;241m=\u001b[39mdropna)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:1063\u001b[0m, in \u001b[0;36mIndexOpsMixin.nunique\u001b[0;34m(self, dropna)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnunique\u001b[39m(\u001b[38;5;28mself\u001b[39m, dropna: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;124;03m    Return number of unique elements in the object.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    4\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1063\u001b[0m     uniqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropna:\n\u001b[1;32m   1065\u001b[0m         uniqs \u001b[38;5;241m=\u001b[39m remove_na_arraylike(uniqs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/series.py:2407\u001b[0m, in \u001b[0;36mSeries.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ArrayLike:  \u001b[38;5;66;03m# pylint: disable=useless-parent-delegation\u001b[39;00m\n\u001b[1;32m   2345\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2346\u001b[0m \u001b[38;5;124;03m    Return unique values of Series object.\u001b[39;00m\n\u001b[1;32m   2347\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[38;5;124;03m    Categories (3, object): ['a' < 'b' < 'c']\u001b[39;00m\n\u001b[1;32m   2406\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/base.py:1025\u001b[0m, in \u001b[0;36mIndexOpsMixin.unique\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     result \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39munique()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1025\u001b[0m     result \u001b[38;5;241m=\u001b[39m algorithms\u001b[38;5;241m.\u001b[39munique1d(values)\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:401\u001b[0m, in \u001b[0;36munique\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munique\u001b[39m(values):\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    Return unique values based on a hash table.\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;124;03m    array([('a', 'b'), ('b', 'a'), ('a', 'c')], dtype=object)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_with_mask(values)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/pandas/core/algorithms.py:440\u001b[0m, in \u001b[0;36munique_with_mask\u001b[0;34m(values, mask)\u001b[0m\n\u001b[1;32m    438\u001b[0m table \u001b[38;5;241m=\u001b[39m hashtable(\u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m table\u001b[38;5;241m.\u001b[39munique(values)\n\u001b[1;32m    441\u001b[0m     uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m uniques\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7248\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.unique\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7195\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df_UK_lat_lon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"df_UK_lat_lon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'RLOIid', 'catchmentName', 'dateOpened', 'datumOffset', 'easting',\n",
       "       'label', 'lat', 'long', 'northing', 'notation', 'riverName',\n",
       "       'stageScale', 'stationReference', 'status', 'town', 'wiskiID',\n",
       "       'measures'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5392 entries, 0 to 5391\n",
      "Data columns (total 18 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                5392 non-null   object \n",
      " 1   RLOIid            2526 non-null   object \n",
      " 2   catchmentName     2526 non-null   object \n",
      " 3   dateOpened        2526 non-null   object \n",
      " 4   datumOffset       294 non-null    float64\n",
      " 5   easting           4758 non-null   object \n",
      " 6   label             5392 non-null   object \n",
      " 7   lat               4758 non-null   object \n",
      " 8   long              4758 non-null   object \n",
      " 9   northing          4758 non-null   object \n",
      " 10  notation          5391 non-null   object \n",
      " 11  riverName         2526 non-null   object \n",
      " 12  stageScale        2526 non-null   object \n",
      " 13  stationReference  5391 non-null   object \n",
      " 14  status            2526 non-null   object \n",
      " 15  town              2367 non-null   object \n",
      " 16  wiskiID           2511 non-null   object \n",
      " 17  measures          5392 non-null   object \n",
      "dtypes: float64(1), object(17)\n",
      "memory usage: 758.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
